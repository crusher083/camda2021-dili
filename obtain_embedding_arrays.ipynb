{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "# Deep learning\n",
    "from transformers import AutoTokenizer\n",
    "from loop_train_berts import (\n",
    "    set_seed,\n",
    "    BertClassifier,\n",
    "    preprocessing_for_bert,\n",
    "    initialize_model, \n",
    "    train,\n",
    "    bert_predict\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'allenai/biomed_roberta_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine-tuning BERT, the authors recommend a batch size of 16-32, but our RTX could hold only 8. \n",
    "batch_size = 4\n",
    "\n",
    "positive = pd.read_csv('data/positive.tsv', sep='\\t', index_col=0)\n",
    "positive['target'] = 1\n",
    "negative = pd.read_csv('data/negative.tsv', sep='\\t', index_col=0)\n",
    "negative['target'] = 0\n",
    "data = positive.append(negative)\n",
    "data['concat'] = data.Title.map(str) + \" \" + data.Abstract.fillna(' ').map(str)\n",
    "data['bert'] = data['concat'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_masks, offset = preprocessing_for_bert(tokenizer, data.bert, return_offset=True)\n",
    "train_labels = torch.tensor(data.target.values)\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "set_seed(42)\n",
    "bert_classifier, optimizer, scheduler = initialize_model(model_path, device, train_dataloader, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   100   |   0.448089   |     -      |     -     |   23.82  \n",
      "   1    |   200   |   0.416967   |     -      |     -     |   23.77  \n",
      "   1    |   300   |   0.308454   |     -      |     -     |   24.39  \n",
      "   1    |   400   |   0.388859   |     -      |     -     |   25.01  \n",
      "   1    |   500   |   0.314160   |     -      |     -     |   25.27  \n",
      "   1    |   600   |   0.342056   |     -      |     -     |   26.08  \n",
      "   1    |   700   |   0.191861   |     -      |     -     |   26.53  \n",
      "   1    |   800   |   0.192471   |     -      |     -     |   26.81  \n",
      "   1    |   900   |   0.157770   |     -      |     -     |   27.45  \n",
      "   1    |  1000   |   0.197449   |     -      |     -     |   27.97  \n",
      "   1    |  1100   |   0.218265   |     -      |     -     |   28.43  \n",
      "   1    |  1200   |   0.121481   |     -      |     -     |   28.98  \n",
      "   1    |  1300   |   0.149150   |     -      |     -     |   29.44  \n",
      "   1    |  1400   |   0.236945   |     -      |     -     |   29.99  \n",
      "   1    |  1500   |   0.160824   |     -      |     -     |   30.39  \n",
      "   1    |  1600   |   0.190380   |     -      |     -     |   30.50  \n",
      "   1    |  1700   |   0.104258   |     -      |     -     |   30.78  \n",
      "   1    |  1800   |   0.189081   |     -      |     -     |   31.24  \n",
      "   1    |  1900   |   0.113027   |     -      |     -     |   31.60  \n",
      "   1    |  2000   |   0.187339   |     -      |     -     |   32.07  \n",
      "   1    |  2100   |   0.138811   |     -      |     -     |   32.48  \n",
      "   1    |  2200   |   0.353212   |     -      |     -     |   32.92  \n",
      "   1    |  2300   |   0.240051   |     -      |     -     |   32.92  \n",
      "   1    |  2400   |   0.414282   |     -      |     -     |   33.29  \n",
      "   1    |  2500   |   0.667143   |     -      |     -     |   33.77  \n",
      "   1    |  2600   |   0.496363   |     -      |     -     |   34.45  \n",
      "   1    |  2700   |   0.843027   |     -      |     -     |   35.29  \n",
      "   1    |  2800   |   0.655236   |     -      |     -     |   35.77  \n",
      "   1    |  2900   |   0.346476   |     -      |     -     |   36.06  \n",
      "   1    |  3000   |   0.367890   |     -      |     -     |   36.53  \n",
      "   1    |  3100   |   0.192791   |     -      |     -     |   36.90  \n",
      "   1    |  3200   |   0.359925   |     -      |     -     |   37.45  \n",
      "   1    |  3300   |   0.204001   |     -      |     -     |   37.80  \n",
      "   1    |  3400   |   0.182619   |     -      |     -     |   37.79  \n",
      "   1    |  3500   |   0.228890   |     -      |     -     |   38.43  \n",
      "   1    |  3550   |   0.450805   |     -      |     -     |   19.58  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   100   |   0.290833   |     -      |     -     |   23.68  \n",
      "   2    |   200   |   0.309075   |     -      |     -     |   23.93  \n",
      "   2    |   300   |   0.343089   |     -      |     -     |   24.53  \n",
      "   2    |   400   |   0.353106   |     -      |     -     |   25.06  \n",
      "   2    |   500   |   0.270223   |     -      |     -     |   25.48  \n",
      "   2    |   600   |   0.327211   |     -      |     -     |   25.65  \n",
      "   2    |   700   |   0.349785   |     -      |     -     |   26.47  \n",
      "   2    |   800   |   0.340205   |     -      |     -     |   26.97  \n",
      "   2    |   900   |   0.291370   |     -      |     -     |   27.45  \n",
      "   2    |  1000   |   0.325885   |     -      |     -     |   27.92  \n",
      "   2    |  1100   |   0.243356   |     -      |     -     |   28.33  \n",
      "   2    |  1200   |   0.227471   |     -      |     -     |   28.83  \n",
      "   2    |  1300   |   0.178938   |     -      |     -     |   29.24  \n",
      "   2    |  1400   |   0.221378   |     -      |     -     |   29.70  \n",
      "   2    |  1500   |   0.371058   |     -      |     -     |   30.06  \n",
      "   2    |  1600   |   0.232139   |     -      |     -     |   30.37  \n",
      "   2    |  1700   |   0.267868   |     -      |     -     |   30.68  \n",
      "   2    |  1800   |   0.236511   |     -      |     -     |   31.12  \n",
      "   2    |  1900   |   0.284528   |     -      |     -     |   31.65  \n",
      "   2    |  2000   |   0.337258   |     -      |     -     |   32.09  \n",
      "   2    |  2100   |   0.221711   |     -      |     -     |   32.46  \n",
      "   2    |  2200   |   0.149733   |     -      |     -     |   32.90  \n",
      "   2    |  2300   |   0.223969   |     -      |     -     |   33.45  \n",
      "   2    |  2400   |   0.234796   |     -      |     -     |   33.86  \n",
      "   2    |  2500   |   0.216350   |     -      |     -     |   34.31  \n",
      "   2    |  2600   |   0.150622   |     -      |     -     |   34.79  \n",
      "   2    |  2700   |   0.256994   |     -      |     -     |   35.21  \n",
      "   2    |  2800   |   0.270383   |     -      |     -     |   35.68  \n",
      "   2    |  2900   |   0.140137   |     -      |     -     |   36.02  \n",
      "   2    |  3000   |   0.234342   |     -      |     -     |   36.54  \n",
      "   2    |  3100   |   0.169659   |     -      |     -     |   36.99  \n",
      "   2    |  3200   |   0.229314   |     -      |     -     |   37.48  \n",
      "   2    |  3300   |   0.225624   |     -      |     -     |   37.96  \n",
      "   2    |  3400   |   0.105715   |     -      |     -     |   38.29  \n",
      "   2    |  3500   |   0.155008   |     -      |     -     |   38.74  \n",
      "   2    |  3550   |   0.158878   |     -      |     -     |   19.44  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   100   |   0.100542   |     -      |     -     |   23.59  \n",
      "   3    |   200   |   0.191267   |     -      |     -     |   23.89  \n",
      "   3    |   300   |   0.120477   |     -      |     -     |   24.49  \n",
      "   3    |   400   |   0.184853   |     -      |     -     |   25.02  \n",
      "   3    |   500   |   0.153611   |     -      |     -     |   25.56  \n",
      "   3    |   600   |   0.286423   |     -      |     -     |   26.03  \n",
      "   3    |   700   |   0.397133   |     -      |     -     |   26.54  \n",
      "   3    |   800   |   0.238629   |     -      |     -     |   26.85  \n",
      "   3    |   900   |   0.098079   |     -      |     -     |   27.40  \n",
      "   3    |  1000   |   0.223099   |     -      |     -     |   27.89  \n",
      "   3    |  1100   |   0.178784   |     -      |     -     |   28.29  \n",
      "   3    |  1200   |   0.164519   |     -      |     -     |   28.89  \n",
      "   3    |  1300   |   0.158920   |     -      |     -     |   29.24  \n",
      "   3    |  1400   |   0.158729   |     -      |     -     |   29.77  \n",
      "   3    |  1500   |   0.269384   |     -      |     -     |   30.28  \n",
      "   3    |  1600   |   0.141045   |     -      |     -     |   30.44  \n",
      "   3    |  1700   |   0.123217   |     -      |     -     |   30.63  \n",
      "   3    |  1800   |   0.198392   |     -      |     -     |   31.06  \n",
      "   3    |  1900   |   0.172981   |     -      |     -     |   31.45  \n",
      "   3    |  2000   |   0.133296   |     -      |     -     |   32.00  \n",
      "   3    |  2100   |   0.133122   |     -      |     -     |   32.46  \n",
      "   3    |  2200   |   0.225294   |     -      |     -     |   32.94  \n",
      "   3    |  2300   |   0.243267   |     -      |     -     |   33.43  \n",
      "   3    |  2400   |   0.189978   |     -      |     -     |   33.81  \n",
      "   3    |  2500   |   0.190995   |     -      |     -     |   34.19  \n",
      "   3    |  2600   |   0.134824   |     -      |     -     |   34.63  \n",
      "   3    |  2700   |   0.164055   |     -      |     -     |   35.04  \n",
      "   3    |  2800   |   0.112505   |     -      |     -     |   35.55  \n",
      "   3    |  2900   |   0.108645   |     -      |     -     |   36.00  \n",
      "   3    |  3000   |   0.171661   |     -      |     -     |   36.47  \n",
      "   3    |  3100   |   0.131874   |     -      |     -     |   36.91  \n",
      "   3    |  3200   |   0.076049   |     -      |     -     |   37.39  \n",
      "   3    |  3300   |   0.174223   |     -      |     -     |   37.80  \n",
      "   3    |  3400   |   0.142632   |     -      |     -     |   38.25  \n",
      "   3    |  3500   |   0.201725   |     -      |     -     |   38.64  \n",
      "   3    |  3550   |   0.319842   |     -      |     -     |   19.48  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   100   |   0.137687   |     -      |     -     |   23.47  \n",
      "   4    |   200   |   0.185541   |     -      |     -     |   23.71  \n",
      "   4    |   300   |   0.155648   |     -      |     -     |   24.50  \n",
      "   4    |   400   |   0.096470   |     -      |     -     |   25.00  \n",
      "   4    |   500   |   0.123750   |     -      |     -     |   25.51  \n",
      "   4    |   600   |   0.147560   |     -      |     -     |   25.97  \n",
      "   4    |   700   |   0.082644   |     -      |     -     |   26.42  \n",
      "   4    |   800   |   0.122023   |     -      |     -     |   26.76  \n",
      "   4    |   900   |   0.084847   |     -      |     -     |   26.98  \n",
      "   4    |  1000   |   0.215947   |     -      |     -     |   27.81  \n",
      "   4    |  1100   |   0.148923   |     -      |     -     |   28.23  \n",
      "   4    |  1200   |   0.135644   |     -      |     -     |   28.67  \n",
      "   4    |  1300   |   0.138735   |     -      |     -     |   28.69  \n",
      "   4    |  1400   |   0.185972   |     -      |     -     |   29.50  \n",
      "   4    |  1500   |   0.296091   |     -      |     -     |   30.30  \n",
      "   4    |  1600   |   0.384417   |     -      |     -     |   30.34  \n",
      "   4    |  1700   |   0.306988   |     -      |     -     |   30.68  \n",
      "   4    |  1800   |   0.411961   |     -      |     -     |   30.90  \n",
      "   4    |  1900   |   0.382001   |     -      |     -     |   31.07  \n",
      "   4    |  2000   |   0.324643   |     -      |     -     |   32.09  \n",
      "   4    |  2100   |   0.366016   |     -      |     -     |   32.52  \n",
      "   4    |  2200   |   0.185147   |     -      |     -     |   32.83  \n",
      "   4    |  2300   |   0.208627   |     -      |     -     |   33.35  \n",
      "   4    |  2400   |   0.226283   |     -      |     -     |   33.95  \n",
      "   4    |  2500   |   0.093756   |     -      |     -     |   34.40  \n",
      "   4    |  2600   |   0.126671   |     -      |     -     |   34.84  \n",
      "   4    |  2700   |   0.108665   |     -      |     -     |   35.23  \n",
      "   4    |  2800   |   0.083356   |     -      |     -     |   35.69  \n",
      "   4    |  2900   |   0.055999   |     -      |     -     |   36.11  \n",
      "   4    |  3000   |   0.161946   |     -      |     -     |   36.60  \n",
      "   4    |  3100   |   0.161111   |     -      |     -     |   37.04  \n",
      "   4    |  3200   |   0.155881   |     -      |     -     |   37.49  \n",
      "   4    |  3300   |   0.102267   |     -      |     -     |   37.82  \n",
      "   4    |  3400   |   0.126365   |     -      |     -     |   38.32  \n",
      "   4    |  3500   |   0.097214   |     -      |     -     |   38.79  \n",
      "   4    |  3550   |   0.152568   |     -      |     -     |   19.52  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train(bert_classifier, device, train_dataloader, optimizer, scheduler, epochs=4, save_emb=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dili",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
